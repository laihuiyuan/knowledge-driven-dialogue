KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...

=====================================================================================
================================== Model Training ===================================
=====================================================================================

KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (softmax): Softmax()
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (softmax): Softmax()
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 800, batch_first=True)
    (cue_rnn): GRU(800, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=800, out_features=30004, bias=True)
      (2): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 69491609

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (softmax): Softmax()
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 800, batch_first=True)
    (cue_rnn): GRU(800, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=800, out_features=30004, bias=True)
      (2): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 69491609

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): LSTMEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): LSTMEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): LSTM(1100, 800, batch_first=True)
    (cue_rnn): LSTM(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 79178809

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): LSTM(1100, 800, batch_first=True)
    (cue_rnn): LSTM(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 79178809

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): LSTM(1100, 800, batch_first=True)
    (cue_rnn): LSTM(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 79178809

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): LSTM(1100, 800, batch_first=True)
    (cue_rnn): LSTM(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 79178809

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): LSTM(1100, 800, batch_first=True)
    (cue_rnn): LSTM(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 79178809

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): LSTM(1100, 800, batch_first=True)
    (cue_rnn): LSTM(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 79178809

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): LSTM(1100, 800, batch_first=True)
    (cue_rnn): LSTM(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 79178809

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (kgn_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): LSTM(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (softmax): Softmax()
  (log_softmax): LogSoftmax()
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 75735609

Training starts ...
KnowledgeSeq2Seq(
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (knowledge_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (prior_attention): Attention(800, 800, mode='dot')
  (posterior_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (kng_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (kng_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (kng_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (kng_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
KnowledgeSeq2Seq(
  (encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (decoder): RNNDecoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (attention): Attention(800, 800, mode='dot')
    (rnn): GRU(1100, 800, batch_first=True)
    (cue_rnn): GRU(1600, 800, batch_first=True)
    (fc1): Linear(in_features=800, out_features=800, bias=True)
    (fc2): Linear(in_features=800, out_features=800, bias=True)
    (fc3): Linear(in_features=1600, out_features=1, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (output_layer): Sequential(
      (0): Dropout(p=0.3)
      (1): Linear(in_features=1600, out_features=800, bias=True)
      (2): Linear(in_features=800, out_features=30004, bias=True)
      (3): LogSoftmax()
    )
  )
  (kng_encoder): RNNEncoder(
    (embedder): Embedder(30004, 300, padding_idx=0)
    (rnn): GRU(300, 400, batch_first=True, bidirectional=True)
  )
  (pri_attention): Attention(800, 800, mode='dot')
  (pos_attention): Attention(800, 800, mode='dot')
  (log_softmax): LogSoftmax()
  (softmax): Softmax()
  (sigmoid): Sigmoid()
  (softplus): Softplus(beta=1, threshold=20)
  (bridge): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
  )
  (bow_output_layer): Sequential(
    (0): Linear(in_features=800, out_features=800, bias=True)
    (1): Tanh()
    (2): Linear(in_features=800, out_features=30004, bias=True)
    (3): LogSoftmax()
  )
  (nll_loss): NLLLoss()
  (kl_loss): KLDivLoss()
)
Number of parameters: 74612409

Training starts ...
